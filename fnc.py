# -*- coding: utf-8 -*-
"""FNC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xkBmA2bYoLyZHwBu3okDE7NYLLW_KwB6
"""

# from google.colab import drive
# drive.mount('/content/drive')

import re
import gensim
import numpy as np
import pandas as pd
from keras import Sequential, regularizers
from keras import optimizers
from keras import backend as K
from keras import initializers, constraints, regularizers
from keras.models import Model
from keras.layers import Reshape, Dot, Concatenate, Input, Embedding, Dropout, Dense, LSTM, Bidirectional, Activation
from keras.preprocessing.text import text_to_word_sequence, Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import np_utils, plot_model
from keras.engine.topology import Layer
from sklearn import feature_extraction
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer
import matplotlib as mpl
from matplotlib import pyplot as plt
from IPython.display import Image
import sys
# sys.path.insert(0, './dataset')
from utils.score import report_score, LABELS, score_submission

W2V_DIR = './GoogleNews-vectors-negative300.bin'
DATA_DIR = './dataset'

MAX_SENT_LEN = 150
MAX_FEATURES = 5000
MAX_VOCAB_SIZE = 35000
LSTM_DIM = 128
EMBEDDING_DIM = 300
BATCH_SIZE = 128
N_EPOCHS = 30
np.random.seed(27)

class TFIDF1:
    def __init__(self, W2V_DIR = './GoogleNews-vectors-negative300.bin',
                DATA_DIR = './dataset', MAX_SENT_LEN = 150, MAX_FEATURES = 5000, MAX_VOCAB_SIZE = 35000, LSTM_DIM = 128, 
                 EMBEDDING_DIM = 300, BATCH_SIZE = 128, N_EPOCHS = 90, SEED=np.random.seed(27)):
        self.W2V_DIR = W2V_DIR
        self.DATA_DIR = DATA_DIR
        self.MAX_SENT_LEN = MAX_SENT_LEN
        self.MAX_FEATURES = MAX_FEATURES
        self.MAX_VOCAB_SIZE = MAX_VOCAB_SIZE
        self.LSTM_DIM = LSTM_DIM
        self.EMBEDDING_DIM = EMBEDDING_DIM
        self.BATCH_SIZE = BATCH_SIZE
        self.N_EPOCHS = N_EPOCHS
            
    def load_data(self):
        train_bodies = pd.read_csv(self.DATA_DIR + '/train_bodies.csv')
        train_df = pd.read_csv(self.DATA_DIR + '/train_stances.csv')
        test_bodies = pd.read_csv(self.DATA_DIR + '/competition_test_bodies.csv')
        test_df = pd.read_csv(self.DATA_DIR + '/competition_test_stances.csv')

        combine_train_df = train_df.join(train_bodies.set_index('Body ID'), on='Body ID')
        combine_test_df = test_df.join(test_bodies.set_index('Body ID'), on='Body ID')
        
        self.word_seq_head_train = combine_train_df['Headline'].tolist()
        self.word_seq_bodies_train = combine_train_df['articleBody'].tolist()
        self.word_seq_head_test = combine_test_df['Headline'].tolist()
        self.word_seq_bodies_test = combine_test_df['articleBody'].tolist()
        
        self.word_seq_label_train = combine_train_df['Stance']
        self.word_seq_label_test = combine_test_df['Stance']

        
    def preprocessing(self):
        vectorizer = TfidfVectorizer(max_features=self.MAX_FEATURES)
        vectorizer.fit(self.word_seq_head_train+self.word_seq_bodies_train)
        self.X_train_head = vectorizer.transform(self.word_seq_head_train)
        self.X_train_bodies = vectorizer.transform(self.word_seq_bodies_train)
        self.X_test_head = vectorizer.transform(self.word_seq_head_test)
        self.X_test_bodies = vectorizer.transform(self.word_seq_bodies_test)
        
        
        lb_train = LabelEncoder().fit_transform(self.word_seq_label_train)
        self.Y_train = np_utils.to_categorical(lb_train, num_classes=4)

        lb_test = LabelEncoder().fit_transform(self.word_seq_label_test)
        self.Y_test = np_utils.to_categorical(lb_test, num_classes=4)

        self.X_train_head, self.X_val_head, self.X_train_bodies, self.X_val_bodies, self.Y_train, self.Y_val = train_test_split(self.X_train_head, self.X_train_bodies, self.Y_train, random_state=10, test_size=0.1)

        
    def build(self):
        input_head = Input(shape=(self.MAX_FEATURES,), name='input_head')
        input_bodies = Input(shape=(self.MAX_FEATURES,), name='input_bodies')

        cosin = Dot(axes=-1)([input_head, input_bodies])

        input_concatenated = Concatenate()([input_head, cosin, input_bodies])

        hidden = Dense(100, activation='relu', name='dense1')(input_concatenated)
        hidden = Dropout(rate=0.6, name='dropout')(hidden)
        out = Dense(4, activation='softmax', name='out')(hidden)

        self.model = Model(inputs=[input_head, input_bodies], outputs=out)
        optimizer = optimizers.Adam(lr=0.01)

        self.model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
        
    def fit(self):
        return self.model.fit([self.X_train_head, self.X_train_bodies], self.Y_train, batch_size=self.BATCH_SIZE, epochs=self.N_EPOCHS, validation_data=([self.X_val_head, self.X_val_bodies], self.Y_val))

    def evaluate(self):
        return self.model.evaluate([self.X_test_head, self.X_test_bodies], self.Y_test, batch_size=self.BATCH_SIZE)
    def maxindex(self, a):
        return np.argmax(a)
      
    def score(self):
        predicted = [LABELS[self.maxindex(a)] for a in self.model.predict([self.X_test_head, self.X_test_bodies])]
        actual = [LABELS[self.maxindex(a)] for a in self.Y_test]
        np.savetxt("stance.csv", predicted, delimiter=",", fmt='%s')
        report_score(actual, predicted)

class TFIDF2:
    def __init__(self, W2V_DIR = './GoogleNews-vectors-negative300.bin',
                DATA_DIR = './dataset', MAX_SENT_LEN = 150, MAX_FEATURES = 5000, MAX_VOCAB_SIZE = 35000, LSTM_DIM = 128, 
                 EMBEDDING_DIM = 300, BATCH_SIZE = 128, N_EPOCHS = 90, SEED=np.random.seed(27)):
        self.W2V_DIR = W2V_DIR
        self.DATA_DIR = DATA_DIR
        self.MAX_SENT_LEN = MAX_SENT_LEN
        self.MAX_FEATURES = MAX_FEATURES
        self.MAX_VOCAB_SIZE = MAX_VOCAB_SIZE
        self.LSTM_DIM = LSTM_DIM
        self.EMBEDDING_DIM = EMBEDDING_DIM
        self.BATCH_SIZE = BATCH_SIZE
        self.N_EPOCHS = N_EPOCHS
            
    def load_data(self):
        train_bodies = pd.read_csv(self.DATA_DIR + '/train_bodies.csv')
        train_df = pd.read_csv(self.DATA_DIR + '/train_stances.csv')
        test_bodies = pd.read_csv(self.DATA_DIR + '/competition_test_bodies.csv')
        test_df = pd.read_csv(self.DATA_DIR + '/competition_test_stances.csv')

        combine_train_df = train_df.join(train_bodies.set_index('Body ID'), on='Body ID')
        combine_test_df = test_df.join(test_bodies.set_index('Body ID'), on='Body ID')
        
        self.word_seq_head_train = combine_train_df['Headline'].tolist()
        self.word_seq_bodies_train = combine_train_df['articleBody'].tolist()
        self.word_seq_head_test = combine_test_df['Headline'].tolist()
        self.word_seq_bodies_test = combine_test_df['articleBody'].tolist()
        
        self.word_seq_label_train = combine_train_df['Stance']
        self.word_seq_label_test = combine_test_df['Stance']

        
    def preprocessing(self):
        vectorizer = CountVectorizer(max_features=self.MAX_FEATURES)
        vectorizer.fit(self.word_seq_head_train+self.word_seq_bodies_train)
        X_train_head_count = vectorizer.transform(self.word_seq_head_train)
        X_train_bodies_count = vectorizer.transform(self.word_seq_bodies_train)
        X_test_head_count = vectorizer.transform(self.word_seq_head_test)
        X_test_bodies_count = vectorizer.transform(self.word_seq_bodies_test)
        
        transformer = TfidfTransformer(use_idf=False)
        transformer.fit(X_train_head_count+X_train_bodies_count)
        self.X_train_head_tf = transformer.transform(X_train_head_count)
        self.X_train_bodies_tf = transformer.transform(X_train_bodies_count)
        self.X_test_head_tf = transformer.transform(X_test_head_count)
        self.X_test_bodies_tf = transformer.transform(X_test_bodies_count)
        
        transformer_idf = TfidfTransformer(use_idf=True)
        transformer_idf.fit(X_train_head_count+X_train_bodies_count)
        self.X_train_head_idf = transformer_idf.transform(X_train_head_count)
        self.X_train_bodies_idf = transformer_idf.transform(X_train_bodies_count)
        self.X_test_head_idf = transformer_idf.transform(X_test_head_count)
        self.X_test_bodies_idf = transformer_idf.transform(X_test_bodies_count)
        
        lb_train = LabelEncoder().fit_transform(self.word_seq_label_train)
        self.Y_train = np_utils.to_categorical(lb_train, num_classes=4)

        lb_test = LabelEncoder().fit_transform(self.word_seq_label_test)
        self.Y_test = np_utils.to_categorical(lb_test, num_classes=4)

        self.X_train_head_tf, self.X_val_head_tf, self.X_train_bodies_tf, self.X_val_bodies_tf, self.X_train_head_idf, self.X_val_head_idf, self.X_train_bodies_idf, self.X_val_bodies_idf, self.Y_train, self.Y_val = train_test_split(self.X_train_head_tf, self.X_train_bodies_tf, self.X_train_head_idf, self.X_train_bodies_idf, self.Y_train, random_state=10, test_size=0.1)

        
    def build(self):
        input_head = Input(shape=(self.MAX_FEATURES,), name='input_head')

        input_bodies = Input(shape=(self.MAX_FEATURES,), name='input_bodies')

        input_head_idf = Input(shape=(self.MAX_FEATURES,), name='input_head_idf')

        input_bodies_idf = Input(shape=(self.MAX_FEATURES,), name='input_bodies_idf')

        cosin = Dot(axes=-1)([input_head_idf, input_bodies_idf])

        input_concatenated = Concatenate()([input_head, cosin, input_bodies])

        hidden = Dense(100, activation='relu', name='dense1')(input_concatenated)
        hidden = Dropout(rate=0.6, name='dropout')(hidden)
        out = Dense(4, activation='softmax', name='out')(hidden)

        self.model = Model(inputs=[input_head, input_bodies, input_head_idf, input_bodies_idf], outputs=out)
        optimizer = optimizers.Adam(lr=0.01)
        self.model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
        
    def fit(self):
        return self.model.fit([self.X_train_head_tf, self.X_train_bodies_tf, self.X_train_head_idf, self.X_train_bodies_idf], self.Y_train, batch_size=self.BATCH_SIZE, epochs=self.N_EPOCHS, validation_data=([self.X_val_head_tf, self.X_val_bodies_tf, self.X_val_head_idf, self.X_val_bodies_idf], self.Y_val))
        
    def evaluate(self):
        return self.model.evaluate([self.X_test_head_tf, self.X_test_bodies_tf, self.X_test_head_idf, self.X_test_bodies_idf], self.Y_test, batch_size=self.BATCH_SIZE)    
    
    def maxindex(self, a):
        return np.argmax(a)
      
    def score(self):
        predicted = [LABELS[self.maxindex(a)] for a in self.model.predict([self.X_test_head_tf, self.X_test_bodies_tf, self.X_test_head_idf, self.X_test_bodies_idf])]
        actual = [LABELS[self.maxindex(a)] for a in self.Y_test]
        np.savetxt("stance2.csv", predicted, delimiter=",", fmt='%s')
        report_score(actual, predicted)

class Lstm:
    def __init__(self, W2V_DIR = './GoogleNews-vectors-negative300.bin',
                DATA_DIR = './dataset', MAX_SENT_LEN = 150, MAX_FEATURES = 5000, MAX_VOCAB_SIZE = 35000, LSTM_DIM = 128, 
                 EMBEDDING_DIM = 300, BATCH_SIZE = 128, N_EPOCHS = 40, SEED=np.random.seed(27)):
        self.W2V_DIR = W2V_DIR
        self.DATA_DIR = DATA_DIR
        self.MAX_SENT_LEN = MAX_SENT_LEN
        self.MAX_FEATURES = MAX_FEATURES
        self.MAX_VOCAB_SIZE = MAX_VOCAB_SIZE
        self.LSTM_DIM = LSTM_DIM
        self.EMBEDDING_DIM = EMBEDDING_DIM
        self.BATCH_SIZE = BATCH_SIZE
        self.N_EPOCHS = N_EPOCHS
            
    def clean(self, s):
        s = " ".join(re.findall(r'\w+', s, flags=re.UNICODE)).lower()
        return " ".join([word for word in s.split(" ") if word not in feature_extraction.text.ENGLISH_STOP_WORDS])
      
    def load_data(self):
        train_bodies = pd.read_csv(self.DATA_DIR + '/train_bodies.csv')
        train_df = pd.read_csv(self.DATA_DIR + '/train_stances.csv')
        test_bodies = pd.read_csv(self.DATA_DIR + '/competition_test_bodies.csv')
        test_df = pd.read_csv(self.DATA_DIR + '/competition_test_stances.csv')
        
        self.combine_train_df = train_df.join(train_bodies.set_index('Body ID'), on='Body ID')
        self.combine_test_df = test_df.join(test_bodies.set_index('Body ID'), on='Body ID')

        combine_train_head = [self.clean(head) for head in self.combine_train_df['Headline']]
        combine_train_body = [self.clean(body) for body in self.combine_train_df['articleBody']]
        combine_test_head = [self.clean(head) for head in self.combine_test_df['Headline']]
        combine_test_body = [self.clean(body) for body in self.combine_test_df['articleBody']]

   
        word_seq_head_train = [text_to_word_sequence(head) for head in combine_train_head]
        word_seq_bodies_train = [text_to_word_sequence(body) for body in combine_train_body]
        word_seq_head_test = [text_to_word_sequence(head) for head in combine_test_head]
        word_seq_bodies_test = [text_to_word_sequence(body) for body in combine_test_body]

        
        self.word_seq_train = [None]*len(word_seq_head_train)
        for i in range(len(word_seq_head_train)):
            self.word_seq_train[i] = word_seq_head_train[i] + word_seq_bodies_train[i]

        self.word_seq_test = [None]*len(word_seq_head_test)
        for i in range(len(word_seq_head_test)):
            self.word_seq_test[i] = word_seq_head_test[i] + word_seq_bodies_test[i]
         
    def preprocessing(self):
        word_seq = self.word_seq_train + self.word_seq_test
        self.tokenizer = Tokenizer(num_words=self.MAX_VOCAB_SIZE)
        self.tokenizer.fit_on_texts([' '.join(seq[:self.MAX_SENT_LEN]) for seq in word_seq])
        
        self.X_train = self.tokenizer.texts_to_sequences([' '.join(seq[:self.MAX_SENT_LEN]) for seq in self.word_seq_train])
        self.X_train = pad_sequences(self.X_train, maxlen=self.MAX_SENT_LEN, padding='post', truncating='post')
        lb_train = LabelEncoder().fit_transform(self.combine_train_df['Stance'])
        self.Y_train = np_utils.to_categorical(lb_train, num_classes=4)

        self.X_test = self.tokenizer.texts_to_sequences([' '.join(seq[:self.MAX_SENT_LEN]) for seq in self.word_seq_test])
        self.X_test = pad_sequences(self.X_test, maxlen=self.MAX_SENT_LEN, padding='post', truncating='post')
        lb_test = LabelEncoder().fit_transform(self.combine_test_df['Stance'])
        self.Y_test = np_utils.to_categorical(lb_test, num_classes=4)

        self.X_train, self.X_val, self.Y_train, self.Y_val = train_test_split(self.X_train, self.Y_train, random_state=10, test_size=0.1)

      
    def build(self):
        embeddings = gensim.models.KeyedVectors.load_word2vec_format(self.W2V_DIR, binary=True)
        embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(self.tokenizer.word_index) + 1, self.EMBEDDING_DIM))
        for word, i in self.tokenizer.word_index.items():
            try:
                embeddings_vector = embeddings[word]
            except KeyError:
                embeddings_vector = None
            if embeddings_vector is not None:
                embeddings_matrix[i] = embeddings_vector
        del embeddings
        
        inp = Input(shape=(self.MAX_SENT_LEN, ), name='input_head_and_bodies')
        x = Embedding(len(self.tokenizer.word_index)+1, self.EMBEDDING_DIM, weights=[embeddings_matrix], trainable=False)(inp)
        x = LSTM(self.LSTM_DIM, return_sequences=False, dropout=0.25, recurrent_dropout=0.25)(x)
        x = Dense(64, activation="relu")(x)
        x = Dropout(0.25)(x)
        x = Dense(4, activation="softmax")(x)
        self.model = Model(inputs=inp, outputs=x)
        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    def fit(self):
        return self.model.fit(self.X_train, self.Y_train, batch_size=self.BATCH_SIZE, epochs=self.N_EPOCHS, validation_data=(self.X_val, self.Y_val))
            
    def evaluate(self):
        return self.model.evaluate(self.X_test, self.Y_test, batch_size=self.BATCH_SIZE)    
        
    def maxindex(self, a):
        return np.argmax(a)
        
    def score(self):
        predicted = [LABELS[self.maxindex(a)] for a in self.model.predict(self.X_test)]
        actual = [LABELS[self.maxindex(a)] for a in self.Y_test]
        np.savetxt("stance3.csv", predicted, delimiter=",", fmt='%s')
        report_score(actual, predicted)

class Attention(Layer):
    def __init__(self, step_dim,
                 W_regularizer=None, b_regularizer=None,
                 W_constraint=None, b_constraint=None,
                 bias=True, **kwargs):
        self.supports_masking = True
        self.init = initializers.get('glorot_uniform')

        self.W_regularizer = regularizers.get(W_regularizer)
        self.b_regularizer = regularizers.get(b_regularizer)

        self.W_constraint = constraints.get(W_constraint)
        self.b_constraint = constraints.get(b_constraint)

        self.bias = bias
        self.step_dim = step_dim
        self.features_dim = 128
        super(Attention, self).__init__(**kwargs)

    def build(self, input_shape):
        assert len(input_shape) == 3

        self.W = self.add_weight((input_shape[-1],),
                                 initializer=self.init,
                                 name='{}_W'.format(self.name),
                                 regularizer=self.W_regularizer,
                                 constraint=self.W_constraint)
        self.features_dim = input_shape[-1]

        if self.bias:
            self.b = self.add_weight((input_shape[1],),
                                     initializer='zero',
                                     name='{}_b'.format(self.name),
                                     regularizer=self.b_regularizer,
                                     constraint=self.b_constraint)
        else:
            self.b = None

        self.built = True

    def compute_mask(self, input, input_mask=None):
        return None

    def call(self, x, mask=None):
        features_dim = self.features_dim
        step_dim = self.step_dim

        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),
                              K.reshape(self.W, (features_dim, 1))), (-1, step_dim))

        if self.bias:
            eij += self.b

        eij = K.tanh(eij)

        a = K.exp(eij)

        if mask is not None:
            a *= K.cast(mask, K.floatx())

        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())

        a = K.expand_dims(a)
        weighted_input = x * a
        return K.sum(weighted_input, axis=1)

    def compute_output_shape(self, input_shape):
        return input_shape[0], self.features_dim

class Lstmattention:
    def __init__(self, W2V_DIR = './GoogleNews-vectors-negative300.bin',
                DATA_DIR = './dataset', MAX_SENT_LEN = 150, MAX_FEATURES = 5000, MAX_VOCAB_SIZE = 35000, LSTM_DIM = 128, 
                 EMBEDDING_DIM = 300, BATCH_SIZE = 128, N_EPOCHS = 40, SEED=np.random.seed(27)): 
        self.W2V_DIR = W2V_DIR
        self.DATA_DIR = DATA_DIR
        self.MAX_SENT_LEN = MAX_SENT_LEN
        self.MAX_FEATURES = MAX_FEATURES
        self.MAX_VOCAB_SIZE = MAX_VOCAB_SIZE
        self.LSTM_DIM = LSTM_DIM
        self.EMBEDDING_DIM = EMBEDDING_DIM
        self.BATCH_SIZE = BATCH_SIZE
        self.N_EPOCHS = N_EPOCHS
            
    def clean(self, s):
        s = " ".join(re.findall(r'\w+', s, flags=re.UNICODE)).lower()
        return " ".join([word for word in s.split(" ") if word not in feature_extraction.text.ENGLISH_STOP_WORDS])
      
    def load_data(self):
        train_bodies = pd.read_csv(self.DATA_DIR + '/train_bodies.csv')
        train_df = pd.read_csv(self.DATA_DIR + '/train_stances.csv')
        test_bodies = pd.read_csv(self.DATA_DIR + '/competition_test_bodies.csv')
        test_df = pd.read_csv(self.DATA_DIR + '/competition_test_stances.csv')
        
        self.combine_train_df = train_df.join(train_bodies.set_index('Body ID'), on='Body ID')
        self.combine_test_df = test_df.join(test_bodies.set_index('Body ID'), on='Body ID')

        combine_train_head = [self.clean(head) for head in self.combine_train_df['Headline']]
        combine_train_body = [self.clean(body) for body in self.combine_train_df['articleBody']]
        combine_test_head = [self.clean(head) for head in self.combine_test_df['Headline']]
        combine_test_body = [self.clean(body) for body in self.combine_test_df['articleBody']]

   
        word_seq_head_train = [text_to_word_sequence(head) for head in combine_train_head]
        word_seq_bodies_train = [text_to_word_sequence(body) for body in combine_train_body]
        word_seq_head_test = [text_to_word_sequence(head) for head in combine_test_head]
        word_seq_bodies_test = [text_to_word_sequence(body) for body in combine_test_body]

        
        self.word_seq_train = [None]*len(word_seq_head_train)
        for i in range(len(word_seq_head_train)):
            self.word_seq_train[i] = word_seq_head_train[i] + word_seq_bodies_train[i]

        self.word_seq_test = [None]*len(word_seq_head_test)
        for i in range(len(word_seq_head_test)):
            self.word_seq_test[i] = word_seq_head_test[i] + word_seq_bodies_test[i]
         
    def preprocessing(self):
        word_seq = self.word_seq_train + self.word_seq_test
        self.tokenizer = Tokenizer(num_words=self.MAX_VOCAB_SIZE)
        self.tokenizer.fit_on_texts([' '.join(seq[:self.MAX_SENT_LEN]) for seq in word_seq])
        
        self.X_train = self.tokenizer.texts_to_sequences([' '.join(seq[:self.MAX_SENT_LEN]) for seq in self.word_seq_train])
        self.X_train = pad_sequences(self.X_train, maxlen=self.MAX_SENT_LEN, padding='post', truncating='post')
        lb_train = LabelEncoder().fit_transform(self.combine_train_df['Stance'])
        self.Y_train = np_utils.to_categorical(lb_train, num_classes=4)

        self.X_test = self.tokenizer.texts_to_sequences([' '.join(seq[:self.MAX_SENT_LEN]) for seq in self.word_seq_test])
        self.X_test = pad_sequences(self.X_test, maxlen=self.MAX_SENT_LEN, padding='post', truncating='post')
        lb_test = LabelEncoder().fit_transform(self.combine_test_df['Stance'])
        self.Y_test = np_utils.to_categorical(lb_test, num_classes=4)

        self.X_train, self.X_val, self.Y_train, self.Y_val = train_test_split(self.X_train, self.Y_train, random_state=10, test_size=0.1)

        
    def build(self):
        embeddings = gensim.models.KeyedVectors.load_word2vec_format(self.W2V_DIR, binary=True)
        embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(self.tokenizer.word_index) + 1, self.EMBEDDING_DIM))
        for word, i in self.tokenizer.word_index.items():
            try:
                embeddings_vector = embeddings[word]
            except KeyError:
                embeddings_vector = None
            if embeddings_vector is not None:
                embeddings_matrix[i] = embeddings_vector
        del embeddings
        
        inp = Input(shape=(self.MAX_SENT_LEN, ), name='input_head_and_bodies')
        x = Embedding(len(self.tokenizer.word_index)+1, self.EMBEDDING_DIM, weights=[embeddings_matrix], trainable=False)(inp)
        x = LSTM(self.LSTM_DIM, return_sequences=True, dropout=0.25, recurrent_dropout=0.25)(x)
        x = Attention(self.MAX_SENT_LEN)(x)
        x = Dense(64, activation="relu")(x)
        x = Dropout(0.25)(x)
        x = Dense(4, activation="softmax")(x)
        self.model = Model(inputs=inp, outputs=x)
        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    def fit(self):
        return self.model.fit(self.X_train, self.Y_train, batch_size=self.BATCH_SIZE, epochs=self.N_EPOCHS, validation_data=(self.X_val, self.Y_val))
            
    def evaluate(self):
        return self.model.evaluate(self.X_test, self.Y_test, batch_size=self.BATCH_SIZE)  
        
    def maxindex(self, a):
        return np.argmax(a)
      
    def score(self):
        predicted = [LABELS[self.maxindex(a)] for a in self.model.predict(self.X_test)]
        actual = [LABELS[self.maxindex(a)] for a in self.Y_test]
        np.savetxt("stance4.csv", predicted, delimiter=",", fmt='%s')
        report_score(actual, predicted)

t1 = TFIDF1()
t1.load_data()
t1.preprocessing()
t1.build()
hitstory_1 = t1.fit()
score_1, acc_1 = t1.evaluate()
print("TFIDF1 Model Accuracy on Test Set = {0:4.3f}".format(acc_1))
FNC_score1 = t1.score()
print("Score on Test Set" + str(FNC_score1))
del t1

t2 = TFIDF2()
t2.load_data()
t2.preprocessing()
t2.build()
hitstory_2 = t2.fit()
score_2, acc_2 = t2.evaluate()
print("TFIDF2 Model Accuracy on Test Set = {0:4.3f}".format(acc_2))
FNC_score2 = t2.score()
del t2

l1 = Lstm()
l1.load_data()
l1.preprocessing()
l1.build()
hitstory_3 = l1.fit()
score_3, acc_3 = l1.evaluate()
print("LSTM Model Accuracy on Test Set = {0:4.3f}".format(acc_3))
FNC_score3 =l1.score()
del l1

l2 = Lstmattention()
l2.load_data()
l2.preprocessing()
l2.build()
hitstory_4 = l2.fit()
score_4, acc_4 = l2.evaluate()
print("LSTM With Attention Model Accuracy on Test Set = {0:4.3f}".format(acc_4))
FNC_score4 =l2.score()
del l2

import matplotlib.pyplot as plt
plt.plot(hitstory_1.history['val_acc'], 'r')
plt.plot(hitstory_2.history['val_acc'], 'b')
plt.plot(hitstory_3.history['val_acc'], 'g')
plt.plot(hitstory_4.history['val_acc'], 'c')
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['TFIDF1', 'TFIDF2', 'LSTM', 'LSTM_Attention'], loc='upper left')
plt.show()

